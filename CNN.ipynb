{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV File\n",
    "df = pd.read_csv('GroundTruth\\GroundTruth.csv', dtype = str)\n",
    "\n",
    "# Update Image Column title to include '.jpg'\n",
    "df['image'] = df['image'].apply(lambda x: x+ '.jpg')\n",
    "\n",
    "# Create New Dataframe for Neural Network Training\n",
    "labels=['MEL', 'NV', 'BCC', 'AKIEC', 'BKL', 'DF', 'VASC']\n",
    "label_list=[]\n",
    "for i in range (len(df)):\n",
    "    row = list(df.iloc[i])\n",
    "    del row[0]\n",
    "    index=np.argmax(row)\n",
    "    label = labels[index]\n",
    "    label_list.append(label)\n",
    "df['label'] = label_list\n",
    "df = df.drop(labels, axis=1)\n",
    " #print (df.head()) Validate Dataset is correct\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into Test/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Images and Labels into Test & Training Datasets\n",
    "train_split= .8 # set this to the percentof the data you want to use for training\n",
    "valid_split= .15 # set this to the percent of the data you want to use for validation\n",
    "test_split = 0.05 # percentage of data used for testing\n",
    "\n",
    "train_df, dummy_df = train_test_split(df, train_size = train_split, shuffle = True, random_state = 7261)\n",
    "test_df, valid_df = train_test_split(dummy_df, train_size = test_split, shuffle = True, random_state = 7261)\n",
    "\n",
    "# Show number of each class present in each dataset\n",
    "#print(' train_df length: ', len(train_df), '  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))  \n",
    "#print (train_df.head())\n",
    "#print (train_df['label'].value_counts())\n",
    "#print (train_df['image'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Images & Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Working Directory to Image Folder -> Overcomes string error in Display\n",
    "os.chdir(r'C:\\Users\\Drewster26\\Desktop\\Skin Lesion Project\\Skin-Legion-Classification\\images')\n",
    "\n",
    "# Display First 25 images from training set\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    file_path = str(train_df.iloc[i]['image'])\n",
    "\n",
    "    img = plt.imread(file_path)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(str(train_df.iloc[i]['label']))\n",
    "plt.show()\n",
    "\n",
    "# Change Working Directory Back to Main -> Overcomes string error in Display\n",
    "os.chdir(r'C:\\Users\\Drewster26\\Desktop\\Skin Lesion Project\\Skin-Legion-Classification')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# *Update Image Storage for training *\n",
    "\n",
    "'''\n",
    "# Create Convolutional Base\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(600, 450, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Add Dense layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# Compile and Train Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_df.loc[:,['image']], train_df.loc[:,['label']], epochs=10, \n",
    "                    validation_data=(valid_df.loc[:,['image']], valid_df.loc[:,['label']]))\n",
    "\n",
    "# Training Plot\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Evaluate Model\n",
    "test_loss, test_acc = model.evaluate(test_df.loc[:,['image']], test_df.loc[:,['label']], verbose=2)\n",
    "\n",
    "print(test_acc)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
